{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMkAYZeZONr29zD5rAAmB9p",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/yhanyi/MLNotebooks/blob/main/Tokenization.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# GPT Tokenization Lecture\n",
        "- Andrej Karpathy's video @ https://youtu.be/zduSFxRajkE?si=lB2uuYe0Rg1K6FgN"
      ],
      "metadata": {
        "id": "tr0cxbj_4vtR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "string = \"ÏïàÎÖïÌïòÏÑ∏Ïöî üëã (hello in Korean!)\""
      ],
      "metadata": {
        "id": "xyFUV3Sw45Ri"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Strings are immutable sequences of Unicode code points."
      ],
      "metadata": {
        "id": "9M3bIAT25gqE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "[ord(x) for x in string]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wsIo0tw-5Ua8",
        "outputId": "f964ee45-e66d-4d3f-b54d-1015f09b4537"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[50504,\n",
              " 45397,\n",
              " 54616,\n",
              " 49464,\n",
              " 50836,\n",
              " 32,\n",
              " 128075,\n",
              " 32,\n",
              " 40,\n",
              " 104,\n",
              " 101,\n",
              " 108,\n",
              " 108,\n",
              " 111,\n",
              " 32,\n",
              " 105,\n",
              " 110,\n",
              " 32,\n",
              " 75,\n",
              " 111,\n",
              " 114,\n",
              " 101,\n",
              " 97,\n",
              " 110,\n",
              " 33,\n",
              " 41]"
            ]
          },
          "metadata": {},
          "execution_count": 2
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "UTF-8: Takes every code point and translates to a bytestream, which is between 1-4 bytes."
      ],
      "metadata": {
        "id": "BV5QF-eb5-no"
      }
    },
    {
      "cell_type": "code",
      "source": [
        " string.encode(\"utf-8\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_Z7pNDSa5ZcF",
        "outputId": "8794247f-ec74-4334-d685-81bb571425a8"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "b'\\xec\\x95\\x88\\xeb\\x85\\x95\\xed\\x95\\x98\\xec\\x84\\xb8\\xec\\x9a\\x94 \\xf0\\x9f\\x91\\x8b (hello in Korean!)'"
            ]
          },
          "metadata": {},
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "list(string.encode(\"utf-8\"))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "a47kKVYA6c8y",
        "outputId": "cbac970e-74de-472e-ee4c-d11b90faea95"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[236,\n",
              " 149,\n",
              " 136,\n",
              " 235,\n",
              " 133,\n",
              " 149,\n",
              " 237,\n",
              " 149,\n",
              " 152,\n",
              " 236,\n",
              " 132,\n",
              " 184,\n",
              " 236,\n",
              " 154,\n",
              " 148,\n",
              " 32,\n",
              " 240,\n",
              " 159,\n",
              " 145,\n",
              " 139,\n",
              " 32,\n",
              " 40,\n",
              " 104,\n",
              " 101,\n",
              " 108,\n",
              " 108,\n",
              " 111,\n",
              " 32,\n",
              " 105,\n",
              " 110,\n",
              " 32,\n",
              " 75,\n",
              " 111,\n",
              " 114,\n",
              " 101,\n",
              " 97,\n",
              " 110,\n",
              " 33,\n",
              " 41]"
            ]
          },
          "metadata": {},
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Byte Pair Encoding: Compresses byte sequences to a variable amount.\n",
        "\n",
        "- Iteratively find pairs of tokens that occur the most frequently.\n",
        "- Identify and replace with a new token to append to vocabulary.\n",
        "- For example: \"aaabdaaabac\" can be converted to \"ZabdZabac\" which is then converted to \"ZYdZYac\".\n",
        "- Might even continue recursively to give \"XdXac\""
      ],
      "metadata": {
        "id": "h8VG8Lel6_9k"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "text = \"My name is Yoshikage Kira. I'm 33 years old. My house is in the northeast section of Morioh, where all the villas are, and I am not married. I work as an employee for the Kame Yu department stores, and I get home every day by 8 PM at the latest. I don't smoke, but I occasionally drink. I'm in bed by 11 PM, and make sure I get eight hours of sleep, no matter what. After having a glass of warm milk and doing about twenty minutes of stretches before going to bed, I usually have no problems sleeping until morning. Just like a baby, I wake up without any fatigue or stress in the morning. I was told there were no issues at my last check-up. I'm trying to explain that I'm a person who wishes to live a very quiet life. I take care not to trouble myself with any enemies, like winning and losing, that would cause me to lose sleep at night. That is how I deal with society, and I know that is what brings me happiness. Although, if I were to fight I wouldn't lose to anyone.\"\n",
        "tokens = list(map(int, text.encode(\"utf-8\")))\n",
        "print(tokens)\n",
        "print(len(tokens))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kGzcWpGk6gR4",
        "outputId": "b2b7cf9e-0efb-4d44-8128-78333c471a6f"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[77, 121, 32, 110, 97, 109, 101, 32, 105, 115, 32, 89, 111, 115, 104, 105, 107, 97, 103, 101, 32, 75, 105, 114, 97, 46, 32, 73, 39, 109, 32, 51, 51, 32, 121, 101, 97, 114, 115, 32, 111, 108, 100, 46, 32, 77, 121, 32, 104, 111, 117, 115, 101, 32, 105, 115, 32, 105, 110, 32, 116, 104, 101, 32, 110, 111, 114, 116, 104, 101, 97, 115, 116, 32, 115, 101, 99, 116, 105, 111, 110, 32, 111, 102, 32, 77, 111, 114, 105, 111, 104, 44, 32, 119, 104, 101, 114, 101, 32, 97, 108, 108, 32, 116, 104, 101, 32, 118, 105, 108, 108, 97, 115, 32, 97, 114, 101, 44, 32, 97, 110, 100, 32, 73, 32, 97, 109, 32, 110, 111, 116, 32, 109, 97, 114, 114, 105, 101, 100, 46, 32, 73, 32, 119, 111, 114, 107, 32, 97, 115, 32, 97, 110, 32, 101, 109, 112, 108, 111, 121, 101, 101, 32, 102, 111, 114, 32, 116, 104, 101, 32, 75, 97, 109, 101, 32, 89, 117, 32, 100, 101, 112, 97, 114, 116, 109, 101, 110, 116, 32, 115, 116, 111, 114, 101, 115, 44, 32, 97, 110, 100, 32, 73, 32, 103, 101, 116, 32, 104, 111, 109, 101, 32, 101, 118, 101, 114, 121, 32, 100, 97, 121, 32, 98, 121, 32, 56, 32, 80, 77, 32, 97, 116, 32, 116, 104, 101, 32, 108, 97, 116, 101, 115, 116, 46, 32, 73, 32, 100, 111, 110, 39, 116, 32, 115, 109, 111, 107, 101, 44, 32, 98, 117, 116, 32, 73, 32, 111, 99, 99, 97, 115, 105, 111, 110, 97, 108, 108, 121, 32, 100, 114, 105, 110, 107, 46, 32, 73, 39, 109, 32, 105, 110, 32, 98, 101, 100, 32, 98, 121, 32, 49, 49, 32, 80, 77, 44, 32, 97, 110, 100, 32, 109, 97, 107, 101, 32, 115, 117, 114, 101, 32, 73, 32, 103, 101, 116, 32, 101, 105, 103, 104, 116, 32, 104, 111, 117, 114, 115, 32, 111, 102, 32, 115, 108, 101, 101, 112, 44, 32, 110, 111, 32, 109, 97, 116, 116, 101, 114, 32, 119, 104, 97, 116, 46, 32, 65, 102, 116, 101, 114, 32, 104, 97, 118, 105, 110, 103, 32, 97, 32, 103, 108, 97, 115, 115, 32, 111, 102, 32, 119, 97, 114, 109, 32, 109, 105, 108, 107, 32, 97, 110, 100, 32, 100, 111, 105, 110, 103, 32, 97, 98, 111, 117, 116, 32, 116, 119, 101, 110, 116, 121, 32, 109, 105, 110, 117, 116, 101, 115, 32, 111, 102, 32, 115, 116, 114, 101, 116, 99, 104, 101, 115, 32, 98, 101, 102, 111, 114, 101, 32, 103, 111, 105, 110, 103, 32, 116, 111, 32, 98, 101, 100, 44, 32, 73, 32, 117, 115, 117, 97, 108, 108, 121, 32, 104, 97, 118, 101, 32, 110, 111, 32, 112, 114, 111, 98, 108, 101, 109, 115, 32, 115, 108, 101, 101, 112, 105, 110, 103, 32, 117, 110, 116, 105, 108, 32, 109, 111, 114, 110, 105, 110, 103, 46, 32, 74, 117, 115, 116, 32, 108, 105, 107, 101, 32, 97, 32, 98, 97, 98, 121, 44, 32, 73, 32, 119, 97, 107, 101, 32, 117, 112, 32, 119, 105, 116, 104, 111, 117, 116, 32, 97, 110, 121, 32, 102, 97, 116, 105, 103, 117, 101, 32, 111, 114, 32, 115, 116, 114, 101, 115, 115, 32, 105, 110, 32, 116, 104, 101, 32, 109, 111, 114, 110, 105, 110, 103, 46, 32, 73, 32, 119, 97, 115, 32, 116, 111, 108, 100, 32, 116, 104, 101, 114, 101, 32, 119, 101, 114, 101, 32, 110, 111, 32, 105, 115, 115, 117, 101, 115, 32, 97, 116, 32, 109, 121, 32, 108, 97, 115, 116, 32, 99, 104, 101, 99, 107, 45, 117, 112, 46, 32, 73, 39, 109, 32, 116, 114, 121, 105, 110, 103, 32, 116, 111, 32, 101, 120, 112, 108, 97, 105, 110, 32, 116, 104, 97, 116, 32, 73, 39, 109, 32, 97, 32, 112, 101, 114, 115, 111, 110, 32, 119, 104, 111, 32, 119, 105, 115, 104, 101, 115, 32, 116, 111, 32, 108, 105, 118, 101, 32, 97, 32, 118, 101, 114, 121, 32, 113, 117, 105, 101, 116, 32, 108, 105, 102, 101, 46, 32, 73, 32, 116, 97, 107, 101, 32, 99, 97, 114, 101, 32, 110, 111, 116, 32, 116, 111, 32, 116, 114, 111, 117, 98, 108, 101, 32, 109, 121, 115, 101, 108, 102, 32, 119, 105, 116, 104, 32, 97, 110, 121, 32, 101, 110, 101, 109, 105, 101, 115, 44, 32, 108, 105, 107, 101, 32, 119, 105, 110, 110, 105, 110, 103, 32, 97, 110, 100, 32, 108, 111, 115, 105, 110, 103, 44, 32, 116, 104, 97, 116, 32, 119, 111, 117, 108, 100, 32, 99, 97, 117, 115, 101, 32, 109, 101, 32, 116, 111, 32, 108, 111, 115, 101, 32, 115, 108, 101, 101, 112, 32, 97, 116, 32, 110, 105, 103, 104, 116, 46, 32, 84, 104, 97, 116, 32, 105, 115, 32, 104, 111, 119, 32, 73, 32, 100, 101, 97, 108, 32, 119, 105, 116, 104, 32, 115, 111, 99, 105, 101, 116, 121, 44, 32, 97, 110, 100, 32, 73, 32, 107, 110, 111, 119, 32, 116, 104, 97, 116, 32, 105, 115, 32, 119, 104, 97, 116, 32, 98, 114, 105, 110, 103, 115, 32, 109, 101, 32, 104, 97, 112, 112, 105, 110, 101, 115, 115, 46, 32, 65, 108, 116, 104, 111, 117, 103, 104, 44, 32, 105, 102, 32, 73, 32, 119, 101, 114, 101, 32, 116, 111, 32, 102, 105, 103, 104, 116, 32, 73, 32, 119, 111, 117, 108, 100, 110, 39, 116, 32, 108, 111, 115, 101, 32, 116, 111, 32, 97, 110, 121, 111, 110, 101, 46]\n",
            "975\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def get_pairs(tokens):\n",
        "  counts = {}\n",
        "  for p in zip(tokens, tokens[1:]):\n",
        "    counts[p] = counts.get(p, 0) + 1\n",
        "  return counts\n",
        "\n",
        "pairs = get_pairs(tokens)\n",
        "print(sorted(((v, k) for k, v in pairs.items()), reverse=True))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "w2ON0V9Q-6bC",
        "outputId": "37519c76-09d8-4987-e119-9125cb2ff9d7"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[(32, (101, 32)), (24, (116, 32)), (22, (32, 97)), (21, (32, 116)), (18, (105, 110)), (18, (32, 73)), (17, (115, 32)), (17, (32, 119)), (14, (116, 104)), (14, (73, 32)), (13, (121, 32)), (12, (97, 116)), (12, (46, 32)), (12, (44, 32)), (11, (114, 101)), (11, (111, 32)), (11, (104, 101)), (11, (32, 109)), (10, (110, 103)), (10, (97, 110)), (10, (32, 115)), (9, (116, 111)), (9, (111, 114)), (9, (104, 97)), (9, (101, 115)), (9, (101, 114)), (9, (100, 32)), (9, (32, 108)), (9, (32, 105)), (8, (111, 117)), (8, (32, 110)), (8, (32, 98)), (7, (115, 116)), (7, (110, 111)), (7, (110, 32)), (7, (104, 111)), (7, (97, 115)), (7, (32, 111)), (7, (32, 104)), (6, (115, 101)), (6, (110, 100)), (6, (109, 101)), (6, (109, 32)), (6, (107, 101)), (6, (105, 115)), (6, (103, 32)), (6, (102, 32)), (6, (97, 114)), (6, (32, 100)), (5, (119, 105)), (5, (111, 110)), (5, (108, 101)), (5, (108, 97)), (5, (101, 116)), (5, (32, 101)), (4, (119, 104)), (4, (118, 101)), (4, (117, 116)), (4, (117, 115)), (4, (116, 114)), (4, (116, 101)), (4, (115, 115)), (4, (114, 105)), (4, (114, 32)), (4, (111, 115)), (4, (111, 102)), (4, (110, 105)), (4, (108, 111)), (4, (108, 108)), (4, (108, 105)), (4, (108, 100)), (4, (105, 103)), (4, (105, 101)), (4, (103, 104)), (4, (101, 112)), (4, (101, 101)), (4, (97, 108)), (4, (97, 32)), (4, (73, 39)), (4, (39, 109)), (4, (32, 103)), (3, (119, 111)), (3, (119, 101)), (3, (119, 97)), (3, (116, 105)), (3, (116, 46)), (3, (115, 117)), (3, (115, 108)), (3, (114, 121)), (3, (114, 115)), (3, (110, 121)), (3, (110, 116)), (3, (110, 101)), (3, (109, 111)), (3, (109, 105)), (3, (109, 97)), (3, (108, 32)), (3, (105, 116)), (3, (105, 111)), (3, (105, 108)), (3, (105, 107)), (3, (104, 116)), (3, (103, 101)), (3, (101, 110)), (3, (101, 109)), (3, (101, 100)), (3, (101, 97)), (3, (99, 97)), (3, (98, 121)), (3, (98, 101)), (3, (97, 109)), (3, (97, 107)), (3, (32, 117)), (3, (32, 102)), (3, (32, 99)), (2, (121, 101)), (2, (121, 44)), (2, (119, 32)), (2, (118, 105)), (2, (117, 114)), (2, (117, 112)), (2, (117, 108)), (2, (117, 101)), (2, (116, 121)), (2, (115, 111)), (2, (115, 105)), (2, (115, 104)), (2, (115, 44)), (2, (114, 116)), (2, (114, 111)), (2, (114, 110)), (2, (112, 108)), (2, (112, 105)), (2, (112, 32)), (2, (111, 119)), (2, (111, 116)), (2, (111, 108)), (2, (111, 105)), (2, (111, 99)), (2, (110, 97)), (2, (110, 39)), (2, (109, 121)), (2, (108, 121)), (2, (107, 32)), (2, (105, 102)), (2, (104, 44)), (2, (104, 32)), (2, (103, 46)), (2, (102, 111)), (2, (101, 99)), (2, (101, 46)), (2, (101, 44)), (2, (100, 111)), (2, (100, 101)), (2, (100, 46)), (2, (99, 104)), (2, (98, 108)), (2, (97, 118)), (2, (97, 98)), (2, (80, 77)), (2, (77, 121)), (2, (39, 116)), (2, (32, 118)), (2, (32, 112)), (2, (32, 89)), (2, (32, 80)), (2, (32, 77)), (2, (32, 75)), (2, (32, 65)), (1, (121, 115)), (1, (121, 111)), (1, (121, 105)), (1, (120, 112)), (1, (117, 110)), (1, (117, 105)), (1, (117, 103)), (1, (117, 98)), (1, (117, 97)), (1, (117, 32)), (1, (116, 119)), (1, (116, 116)), (1, (116, 109)), (1, (116, 99)), (1, (116, 97)), (1, (115, 109)), (1, (115, 46)), (1, (114, 114)), (1, (114, 109)), (1, (114, 107)), (1, (114, 97)), (1, (113, 117)), (1, (112, 114)), (1, (112, 112)), (1, (112, 101)), (1, (112, 97)), (1, (112, 46)), (1, (112, 44)), (1, (111, 121)), (1, (111, 109)), (1, (111, 107)), (1, (111, 104)), (1, (111, 98)), (1, (110, 117)), (1, (110, 110)), (1, (110, 107)), (1, (109, 115)), (1, (109, 112)), (1, (108, 116)), (1, (108, 107)), (1, (108, 102)), (1, (107, 110)), (1, (107, 97)), (1, (107, 46)), (1, (107, 45)), (1, (105, 118)), (1, (105, 114)), (1, (104, 105)), (1, (103, 117)), (1, (103, 115)), (1, (103, 111)), (1, (103, 108)), (1, (103, 44)), (1, (102, 116)), (1, (102, 105)), (1, (102, 101)), (1, (102, 97)), (1, (101, 120)), (1, (101, 118)), (1, (101, 108)), (1, (101, 105)), (1, (101, 102)), (1, (100, 114)), (1, (100, 110)), (1, (100, 97)), (1, (100, 44)), (1, (99, 116)), (1, (99, 107)), (1, (99, 105)), (1, (99, 99)), (1, (98, 117)), (1, (98, 114)), (1, (98, 111)), (1, (98, 97)), (1, (97, 121)), (1, (97, 117)), (1, (97, 112)), (1, (97, 105)), (1, (97, 103)), (1, (97, 46)), (1, (89, 117)), (1, (89, 111)), (1, (84, 104)), (1, (77, 111)), (1, (77, 44)), (1, (77, 32)), (1, (75, 105)), (1, (75, 97)), (1, (74, 117)), (1, (65, 108)), (1, (65, 102)), (1, (56, 32)), (1, (51, 51)), (1, (51, 32)), (1, (49, 49)), (1, (49, 32)), (1, (45, 117)), (1, (32, 121)), (1, (32, 113)), (1, (32, 107)), (1, (32, 84)), (1, (32, 74)), (1, (32, 56)), (1, (32, 51)), (1, (32, 49))]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "top_pair = max(pairs, key=pairs.get)\n",
        "top_pair"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eeyhbkgk_0_n",
        "outputId": "ca58bdac-a2d5-40d8-ef14-07c705f96289"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(101, 32)"
            ]
          },
          "metadata": {},
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def merge(tokens, pair, newtoken):\n",
        "  # In the token list, replace all pairs with newtoken.\n",
        "  newtokens = []\n",
        "  i = 0\n",
        "  while i < len(tokens):\n",
        "    if i < len(tokens) - 1 and tokens[i] == pair[0] and tokens[i+1] == pair[1]:\n",
        "      newtokens.append(newtoken)\n",
        "      i += 2\n",
        "    else:\n",
        "      newtokens.append(tokens[i])\n",
        "      i += 1\n",
        "  return newtokens\n",
        "\n",
        "print(merge(tokens, (101, 32), 256))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EFpJCDJRAQt5",
        "outputId": "ee0bd1db-4290-4dcb-a567-fd94911ad7a6"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[77, 121, 32, 110, 97, 109, 256, 105, 115, 32, 89, 111, 115, 104, 105, 107, 97, 103, 256, 75, 105, 114, 97, 46, 32, 73, 39, 109, 32, 51, 51, 32, 121, 101, 97, 114, 115, 32, 111, 108, 100, 46, 32, 77, 121, 32, 104, 111, 117, 115, 256, 105, 115, 32, 105, 110, 32, 116, 104, 256, 110, 111, 114, 116, 104, 101, 97, 115, 116, 32, 115, 101, 99, 116, 105, 111, 110, 32, 111, 102, 32, 77, 111, 114, 105, 111, 104, 44, 32, 119, 104, 101, 114, 256, 97, 108, 108, 32, 116, 104, 256, 118, 105, 108, 108, 97, 115, 32, 97, 114, 101, 44, 32, 97, 110, 100, 32, 73, 32, 97, 109, 32, 110, 111, 116, 32, 109, 97, 114, 114, 105, 101, 100, 46, 32, 73, 32, 119, 111, 114, 107, 32, 97, 115, 32, 97, 110, 32, 101, 109, 112, 108, 111, 121, 101, 256, 102, 111, 114, 32, 116, 104, 256, 75, 97, 109, 256, 89, 117, 32, 100, 101, 112, 97, 114, 116, 109, 101, 110, 116, 32, 115, 116, 111, 114, 101, 115, 44, 32, 97, 110, 100, 32, 73, 32, 103, 101, 116, 32, 104, 111, 109, 256, 101, 118, 101, 114, 121, 32, 100, 97, 121, 32, 98, 121, 32, 56, 32, 80, 77, 32, 97, 116, 32, 116, 104, 256, 108, 97, 116, 101, 115, 116, 46, 32, 73, 32, 100, 111, 110, 39, 116, 32, 115, 109, 111, 107, 101, 44, 32, 98, 117, 116, 32, 73, 32, 111, 99, 99, 97, 115, 105, 111, 110, 97, 108, 108, 121, 32, 100, 114, 105, 110, 107, 46, 32, 73, 39, 109, 32, 105, 110, 32, 98, 101, 100, 32, 98, 121, 32, 49, 49, 32, 80, 77, 44, 32, 97, 110, 100, 32, 109, 97, 107, 256, 115, 117, 114, 256, 73, 32, 103, 101, 116, 32, 101, 105, 103, 104, 116, 32, 104, 111, 117, 114, 115, 32, 111, 102, 32, 115, 108, 101, 101, 112, 44, 32, 110, 111, 32, 109, 97, 116, 116, 101, 114, 32, 119, 104, 97, 116, 46, 32, 65, 102, 116, 101, 114, 32, 104, 97, 118, 105, 110, 103, 32, 97, 32, 103, 108, 97, 115, 115, 32, 111, 102, 32, 119, 97, 114, 109, 32, 109, 105, 108, 107, 32, 97, 110, 100, 32, 100, 111, 105, 110, 103, 32, 97, 98, 111, 117, 116, 32, 116, 119, 101, 110, 116, 121, 32, 109, 105, 110, 117, 116, 101, 115, 32, 111, 102, 32, 115, 116, 114, 101, 116, 99, 104, 101, 115, 32, 98, 101, 102, 111, 114, 256, 103, 111, 105, 110, 103, 32, 116, 111, 32, 98, 101, 100, 44, 32, 73, 32, 117, 115, 117, 97, 108, 108, 121, 32, 104, 97, 118, 256, 110, 111, 32, 112, 114, 111, 98, 108, 101, 109, 115, 32, 115, 108, 101, 101, 112, 105, 110, 103, 32, 117, 110, 116, 105, 108, 32, 109, 111, 114, 110, 105, 110, 103, 46, 32, 74, 117, 115, 116, 32, 108, 105, 107, 256, 97, 32, 98, 97, 98, 121, 44, 32, 73, 32, 119, 97, 107, 256, 117, 112, 32, 119, 105, 116, 104, 111, 117, 116, 32, 97, 110, 121, 32, 102, 97, 116, 105, 103, 117, 256, 111, 114, 32, 115, 116, 114, 101, 115, 115, 32, 105, 110, 32, 116, 104, 256, 109, 111, 114, 110, 105, 110, 103, 46, 32, 73, 32, 119, 97, 115, 32, 116, 111, 108, 100, 32, 116, 104, 101, 114, 256, 119, 101, 114, 256, 110, 111, 32, 105, 115, 115, 117, 101, 115, 32, 97, 116, 32, 109, 121, 32, 108, 97, 115, 116, 32, 99, 104, 101, 99, 107, 45, 117, 112, 46, 32, 73, 39, 109, 32, 116, 114, 121, 105, 110, 103, 32, 116, 111, 32, 101, 120, 112, 108, 97, 105, 110, 32, 116, 104, 97, 116, 32, 73, 39, 109, 32, 97, 32, 112, 101, 114, 115, 111, 110, 32, 119, 104, 111, 32, 119, 105, 115, 104, 101, 115, 32, 116, 111, 32, 108, 105, 118, 256, 97, 32, 118, 101, 114, 121, 32, 113, 117, 105, 101, 116, 32, 108, 105, 102, 101, 46, 32, 73, 32, 116, 97, 107, 256, 99, 97, 114, 256, 110, 111, 116, 32, 116, 111, 32, 116, 114, 111, 117, 98, 108, 256, 109, 121, 115, 101, 108, 102, 32, 119, 105, 116, 104, 32, 97, 110, 121, 32, 101, 110, 101, 109, 105, 101, 115, 44, 32, 108, 105, 107, 256, 119, 105, 110, 110, 105, 110, 103, 32, 97, 110, 100, 32, 108, 111, 115, 105, 110, 103, 44, 32, 116, 104, 97, 116, 32, 119, 111, 117, 108, 100, 32, 99, 97, 117, 115, 256, 109, 256, 116, 111, 32, 108, 111, 115, 256, 115, 108, 101, 101, 112, 32, 97, 116, 32, 110, 105, 103, 104, 116, 46, 32, 84, 104, 97, 116, 32, 105, 115, 32, 104, 111, 119, 32, 73, 32, 100, 101, 97, 108, 32, 119, 105, 116, 104, 32, 115, 111, 99, 105, 101, 116, 121, 44, 32, 97, 110, 100, 32, 73, 32, 107, 110, 111, 119, 32, 116, 104, 97, 116, 32, 105, 115, 32, 119, 104, 97, 116, 32, 98, 114, 105, 110, 103, 115, 32, 109, 256, 104, 97, 112, 112, 105, 110, 101, 115, 115, 46, 32, 65, 108, 116, 104, 111, 117, 103, 104, 44, 32, 105, 102, 32, 73, 32, 119, 101, 114, 256, 116, 111, 32, 102, 105, 103, 104, 116, 32, 73, 32, 119, 111, 117, 108, 100, 110, 39, 116, 32, 108, 111, 115, 256, 116, 111, 32, 97, 110, 121, 111, 110, 101, 46]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "len(merge(tokens, (101, 32), 256))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LpbMJjvxCXqU",
        "outputId": "2a405451-23e3-4ed9-ddd4-46c6b9b064eb"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "943"
            ]
          },
          "metadata": {},
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Assuming we do 20 merges\n",
        "vocab_size = 276\n",
        "num_merges = vocab_size - 256\n",
        "temp = list(tokens)\n",
        "merges = {}\n",
        "for i in range(num_merges):\n",
        "  newtoken = 256 + i\n",
        "  pairs = get_pairs(temp)\n",
        "  pair = max(pairs, key=pairs.get)\n",
        "  print(f\"Merge {pair} into token {newtoken}\")\n",
        "  temp = merge(temp, pair, newtoken)\n",
        "  merges[pair] = newtoken"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IfDwl9EsCvW7",
        "outputId": "61a8e6b1-54fb-49c4-ea1d-e27f5c5b25b8"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Merge (101, 32) into token 256\n",
            "Merge (116, 32) into token 257\n",
            "Merge (105, 110) into token 258\n",
            "Merge (32, 97) into token 259\n",
            "Merge (32, 116) into token 260\n",
            "Merge (32, 73) into token 261\n",
            "Merge (32, 119) into token 262\n",
            "Merge (121, 32) into token 263\n",
            "Merge (115, 32) into token 264\n",
            "Merge (258, 103) into token 265\n",
            "Merge (111, 114) into token 266\n",
            "Merge (101, 114) into token 267\n",
            "Merge (259, 110) into token 268\n",
            "Merge (104, 97) into token 269\n",
            "Merge (111, 117) into token 270\n",
            "Merge (111, 32) into token 271\n",
            "Merge (46, 261) into token 272\n",
            "Merge (101, 115) into token 273\n",
            "Merge (116, 104) into token 274\n",
            "Merge (97, 115) into token 275\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Original token length:\", len(tokens))\n",
        "print(\"Compressed token length:\", len(temp))\n",
        "cr = len(tokens) / len(temp)\n",
        "print(\"Compression ratio:\", cr)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HS_CdzCeEIDd",
        "outputId": "e293330c-3a11-4521-e950-5c805db82531"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Original token length: 975\n",
            "Compressed token length: 728\n",
            "Compression ratio: 1.3392857142857142\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Decoding: getting the original string given a list of tokens."
      ],
      "metadata": {
        "id": "298QLEAoJuKz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "vocab = {x: bytes([x]) for x in range(256)}\n",
        "for (p0, p1), x in merges.items():\n",
        "  vocab[x] = vocab[p0] + vocab[p1]\n",
        "\n",
        "def decode(ids):\n",
        "  tokens = b\"\".join(vocab[x] for x in ids)\n",
        "  text = tokens.decode(\"utf-8\", errors=\"replace\")\n",
        "  return text"
      ],
      "metadata": {
        "id": "aUwrOtM9Ezz6"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Encoding: getting a list of tokens given a string."
      ],
      "metadata": {
        "id": "QQpNfA9mJ5Sa"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def encode(text):\n",
        "  tokens = list(text.encode(\"utf-8\"))\n",
        "  while len(tokens) > 1:\n",
        "    # Returns the most eligible merge pair that occurs in the tokens.\n",
        "    pairs = get_pairs(tokens)\n",
        "    pair = min(pairs, key=lambda p: pairs.get(p, float(\"inf\")))\n",
        "    if pair not in pairs:\n",
        "      break\n",
        "    idx = pairs[pair]\n",
        "    tokens = merge(tokens, pair, idx)\n",
        "  return tokens"
      ],
      "metadata": {
        "id": "D57v-DBoJ9Vc"
      },
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Forced splits using regex patterns"
      ],
      "metadata": {
        "id": "BnhJqgkRQayE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import regex as re\n",
        "pattern = re.compile(r\"\"\"'s|'t|'re|'ve|'m|'ll|'d| ?\\p{L}+| ?\\p{N}+| ?[^\\s\\p{L}\\p{N}]+|\\s+(?!\\S)|\\s+\"\"\")"
      ],
      "metadata": {
        "id": "npZM41QvM7FR"
      },
      "execution_count": 32,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(re.findall(pattern, \"Hello how's are you'll?!?!?!\"))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ekbCl6cgRa4y",
        "outputId": "10cd84eb-08bd-48e2-b3e6-44ff67a08c69"
      },
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['Hello', ' how', \"'s\", ' are', ' you', \"'ll\", '?!?!?!']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install tiktoken"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JsTaGmr1V-Rs",
        "outputId": "80268029-c77a-49e5-cb5d-8412d8127659"
      },
      "execution_count": 36,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting tiktoken\n",
            "  Downloading tiktoken-0.6.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.8 MB)\n",
            "\u001b[2K     \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m1.8/1.8 MB\u001b[0m \u001b[31m14.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: regex>=2022.1.18 in /usr/local/lib/python3.10/dist-packages (from tiktoken) (2023.12.25)\n",
            "Requirement already satisfied: requests>=2.26.0 in /usr/local/lib/python3.10/dist-packages (from tiktoken) (2.31.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests>=2.26.0->tiktoken) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests>=2.26.0->tiktoken) (3.6)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests>=2.26.0->tiktoken) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests>=2.26.0->tiktoken) (2024.2.2)\n",
            "Installing collected packages: tiktoken\n",
            "Successfully installed tiktoken-0.6.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import tiktoken\n",
        "enc = tiktoken.get_encoding(\"gpt2\")\n",
        "print(enc.encode(\"     hello123's world!?!?!\"))\n",
        "enc = tiktoken.get_encoding(\"cl100k_base\")\n",
        "print(enc.encode(\"     hello123's world!?!?!\"))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yTrZhViqRoTH",
        "outputId": "b611a7b1-e9d3-44e9-e299-7068f77ee95c"
      },
      "execution_count": 37,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[220, 220, 220, 220, 23748, 10163, 338, 995, 0, 12248, 12248]\n",
            "[257, 24748, 4513, 596, 1917, 0, 27074, 27074]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!wget https://openaipublic.blob.core.windows.net/gpt-2/models/1558M/vocab.bpe\n",
        "!wget https://openaipublic.blob.core.windows.net/gpt-2/models/1558M/encoder.json"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LdZrLOLjV4MP",
        "outputId": "e7db1296-2e39-43a4-e6b2-a52df7fa5683"
      },
      "execution_count": 38,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2024-02-24 04:29:34--  https://openaipublic.blob.core.windows.net/gpt-2/models/1558M/vocab.bpe\n",
            "Resolving openaipublic.blob.core.windows.net (openaipublic.blob.core.windows.net)... 20.60.179.33\n",
            "Connecting to openaipublic.blob.core.windows.net (openaipublic.blob.core.windows.net)|20.60.179.33|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 456318 (446K) [application/octet-stream]\n",
            "Saving to: ‚Äòvocab.bpe‚Äô\n",
            "\n",
            "vocab.bpe           100%[===================>] 445.62K   514KB/s    in 0.9s    \n",
            "\n",
            "2024-02-24 04:29:35 (514 KB/s) - ‚Äòvocab.bpe‚Äô saved [456318/456318]\n",
            "\n",
            "--2024-02-24 04:29:35--  https://openaipublic.blob.core.windows.net/gpt-2/models/1558M/encoder.json\n",
            "Resolving openaipublic.blob.core.windows.net (openaipublic.blob.core.windows.net)... 20.60.179.33\n",
            "Connecting to openaipublic.blob.core.windows.net (openaipublic.blob.core.windows.net)|20.60.179.33|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 1042301 (1018K) [application/json]\n",
            "Saving to: ‚Äòencoder.json‚Äô\n",
            "\n",
            "encoder.json        100%[===================>]   1018K   911KB/s    in 1.1s    \n",
            "\n",
            "2024-02-24 04:29:37 (911 KB/s) - ‚Äòencoder.json‚Äô saved [1042301/1042301]\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os, json\n",
        "with open(\"encoder.json\", \"r\") as f:\n",
        "  encoder = json.load(f)\n",
        "\n",
        "with open(\"vocab.bpe\", \"r\", encoding=\"utf-8\") as f:\n",
        "  bpe_data = f.read()\n",
        "bpe_merges = [tuple(merge_str.split()) for merge_str in bpe_data.split(\"\\n\")[1:-1]]"
      ],
      "metadata": {
        "id": "kvg8TWyPYCE3"
      },
      "execution_count": 40,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Special Tokens"
      ],
      "metadata": {
        "id": "BL2gMIaYYfpf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 256 raw byte tokens, 50000 merges, +1 special token\n",
        "len(encoder)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Im_CE-iBYdWP",
        "outputId": "6f018f70-b06b-4c49-b514-e9885b5d6891"
      },
      "execution_count": 41,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "50257"
            ]
          },
          "metadata": {},
          "execution_count": 41
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "encoder[\"<|endoftext|>\"]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pu4mWXT0Ypfh",
        "outputId": "049dce63-54c3-4807-95e1-9a0ed552d672"
      },
      "execution_count": 42,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "50256"
            ]
          },
          "metadata": {},
          "execution_count": 42
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Sentencepiece\n",
        "- Can efficiently both train and inference BPE tokenizers. Used in both llama and mistral.\n",
        "- Tiktoken encodes to utf-8 and BPEs bytes.\n",
        "- Sentencepiece BPEs the code points and optionally falls back to utf-8 bytes for rare code points (rarity is determined by character_coverage hyperparameter) which then gets translate to byte tokens."
      ],
      "metadata": {
        "id": "KSBxncx8bNAS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import sentencepiece as spm"
      ],
      "metadata": {
        "id": "2iOd-ur2aJIo"
      },
      "execution_count": 43,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "with open(\"toy.txt\", \"w\", encoding=\"utf-8\") as f:\n",
        "  f.write(\"My name is Yoshikage Kira. I'm 33 years old. My house is in the northeast section of Morioh, where all the villas are, and I am not married. I work as an employee for the Kame Yu department stores, and I get home every day by 8 PM at the latest. I don't smoke, but I occasionally drink. I'm in bed by 11 PM, and make sure I get eight hours of sleep, no matter what. After having a glass of warm milk and doing about twenty minutes of stretches before going to bed, I usually have no problems sleeping until morning. Just like a baby, I wake up without any fatigue or stress in the morning. I was told there were no issues at my last check-up. I'm trying to explain that I'm a person who wishes to live a very quiet life. I take care not to trouble myself with any enemies, like winning and losing, that would cause me to lose sleep at night. That is how I deal with society, and I know that is what brings me happiness. Although, if I were to fight I wouldn't lose to anyone.\")"
      ],
      "metadata": {
        "id": "9fi8tbROcGR_"
      },
      "execution_count": 44,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "options = dict(\n",
        "  # input spec\n",
        "  input=\"toy.txt\",\n",
        "  input_format=\"text\",\n",
        "  # output spec\n",
        "  model_prefix=\"tok400\", # output filename prefix\n",
        "  # algorithm spec\n",
        "  # BPE alg\n",
        "  model_type=\"bpe\",\n",
        "  vocab_size=400,\n",
        "  # normalization\n",
        "  normalization_rule_name=\"identity\", # ew, turn off normalization\n",
        "  remove_extra_whitespaces=False,\n",
        "  input_sentence_size=200000000, # max number of training sentences\n",
        "  max_sentence_length=4192, # max number of bytes per sentence\n",
        "  seed_sentencepiece_size=1000000,\n",
        "  shuffle_input_sentence=True,\n",
        "  # rare word treatment\n",
        "  character_coverage=0.99995,\n",
        "  byte_fallback=True,\n",
        "  # merge rules\n",
        "  split_digits=True,\n",
        "  split_by_unicode_script=True,\n",
        "  split_by_whitespace=True,\n",
        "  split_by_number=True,\n",
        "  max_sentencepiece_length=16,\n",
        "  add_dummy_prefix=True,\n",
        "  allow_whitespace_only_pieces=True,\n",
        "  # special tokens\n",
        "  unk_id=0, # the UNK token MUST exist\n",
        "  bos_id=1, # the others are optional, set to -1 to turn off\n",
        "  eos_id=2,\n",
        "  pad_id=-1,\n",
        "  # systems\n",
        "  num_threads=os.cpu_count(), # use ~all system resources\n",
        ")\n",
        "\n",
        "spm.SentencePieceTrainer.train(**options)"
      ],
      "metadata": {
        "id": "buw7XWt4gxa8"
      },
      "execution_count": 45,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sp = spm.SentencePieceProcessor()\n",
        "sp.load('tok400.model')\n",
        "vocab = [[sp.id_to_piece(idx), idx] for idx in range(sp.get_piece_size())]\n",
        "vocab"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "J-bMLqEcg270",
        "outputId": "ab1d9d4a-f152-428b-ad1c-58810e46ab65"
      },
      "execution_count": 46,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[['<unk>', 0],\n",
              " ['<s>', 1],\n",
              " ['</s>', 2],\n",
              " ['<0x00>', 3],\n",
              " ['<0x01>', 4],\n",
              " ['<0x02>', 5],\n",
              " ['<0x03>', 6],\n",
              " ['<0x04>', 7],\n",
              " ['<0x05>', 8],\n",
              " ['<0x06>', 9],\n",
              " ['<0x07>', 10],\n",
              " ['<0x08>', 11],\n",
              " ['<0x09>', 12],\n",
              " ['<0x0A>', 13],\n",
              " ['<0x0B>', 14],\n",
              " ['<0x0C>', 15],\n",
              " ['<0x0D>', 16],\n",
              " ['<0x0E>', 17],\n",
              " ['<0x0F>', 18],\n",
              " ['<0x10>', 19],\n",
              " ['<0x11>', 20],\n",
              " ['<0x12>', 21],\n",
              " ['<0x13>', 22],\n",
              " ['<0x14>', 23],\n",
              " ['<0x15>', 24],\n",
              " ['<0x16>', 25],\n",
              " ['<0x17>', 26],\n",
              " ['<0x18>', 27],\n",
              " ['<0x19>', 28],\n",
              " ['<0x1A>', 29],\n",
              " ['<0x1B>', 30],\n",
              " ['<0x1C>', 31],\n",
              " ['<0x1D>', 32],\n",
              " ['<0x1E>', 33],\n",
              " ['<0x1F>', 34],\n",
              " ['<0x20>', 35],\n",
              " ['<0x21>', 36],\n",
              " ['<0x22>', 37],\n",
              " ['<0x23>', 38],\n",
              " ['<0x24>', 39],\n",
              " ['<0x25>', 40],\n",
              " ['<0x26>', 41],\n",
              " ['<0x27>', 42],\n",
              " ['<0x28>', 43],\n",
              " ['<0x29>', 44],\n",
              " ['<0x2A>', 45],\n",
              " ['<0x2B>', 46],\n",
              " ['<0x2C>', 47],\n",
              " ['<0x2D>', 48],\n",
              " ['<0x2E>', 49],\n",
              " ['<0x2F>', 50],\n",
              " ['<0x30>', 51],\n",
              " ['<0x31>', 52],\n",
              " ['<0x32>', 53],\n",
              " ['<0x33>', 54],\n",
              " ['<0x34>', 55],\n",
              " ['<0x35>', 56],\n",
              " ['<0x36>', 57],\n",
              " ['<0x37>', 58],\n",
              " ['<0x38>', 59],\n",
              " ['<0x39>', 60],\n",
              " ['<0x3A>', 61],\n",
              " ['<0x3B>', 62],\n",
              " ['<0x3C>', 63],\n",
              " ['<0x3D>', 64],\n",
              " ['<0x3E>', 65],\n",
              " ['<0x3F>', 66],\n",
              " ['<0x40>', 67],\n",
              " ['<0x41>', 68],\n",
              " ['<0x42>', 69],\n",
              " ['<0x43>', 70],\n",
              " ['<0x44>', 71],\n",
              " ['<0x45>', 72],\n",
              " ['<0x46>', 73],\n",
              " ['<0x47>', 74],\n",
              " ['<0x48>', 75],\n",
              " ['<0x49>', 76],\n",
              " ['<0x4A>', 77],\n",
              " ['<0x4B>', 78],\n",
              " ['<0x4C>', 79],\n",
              " ['<0x4D>', 80],\n",
              " ['<0x4E>', 81],\n",
              " ['<0x4F>', 82],\n",
              " ['<0x50>', 83],\n",
              " ['<0x51>', 84],\n",
              " ['<0x52>', 85],\n",
              " ['<0x53>', 86],\n",
              " ['<0x54>', 87],\n",
              " ['<0x55>', 88],\n",
              " ['<0x56>', 89],\n",
              " ['<0x57>', 90],\n",
              " ['<0x58>', 91],\n",
              " ['<0x59>', 92],\n",
              " ['<0x5A>', 93],\n",
              " ['<0x5B>', 94],\n",
              " ['<0x5C>', 95],\n",
              " ['<0x5D>', 96],\n",
              " ['<0x5E>', 97],\n",
              " ['<0x5F>', 98],\n",
              " ['<0x60>', 99],\n",
              " ['<0x61>', 100],\n",
              " ['<0x62>', 101],\n",
              " ['<0x63>', 102],\n",
              " ['<0x64>', 103],\n",
              " ['<0x65>', 104],\n",
              " ['<0x66>', 105],\n",
              " ['<0x67>', 106],\n",
              " ['<0x68>', 107],\n",
              " ['<0x69>', 108],\n",
              " ['<0x6A>', 109],\n",
              " ['<0x6B>', 110],\n",
              " ['<0x6C>', 111],\n",
              " ['<0x6D>', 112],\n",
              " ['<0x6E>', 113],\n",
              " ['<0x6F>', 114],\n",
              " ['<0x70>', 115],\n",
              " ['<0x71>', 116],\n",
              " ['<0x72>', 117],\n",
              " ['<0x73>', 118],\n",
              " ['<0x74>', 119],\n",
              " ['<0x75>', 120],\n",
              " ['<0x76>', 121],\n",
              " ['<0x77>', 122],\n",
              " ['<0x78>', 123],\n",
              " ['<0x79>', 124],\n",
              " ['<0x7A>', 125],\n",
              " ['<0x7B>', 126],\n",
              " ['<0x7C>', 127],\n",
              " ['<0x7D>', 128],\n",
              " ['<0x7E>', 129],\n",
              " ['<0x7F>', 130],\n",
              " ['<0x80>', 131],\n",
              " ['<0x81>', 132],\n",
              " ['<0x82>', 133],\n",
              " ['<0x83>', 134],\n",
              " ['<0x84>', 135],\n",
              " ['<0x85>', 136],\n",
              " ['<0x86>', 137],\n",
              " ['<0x87>', 138],\n",
              " ['<0x88>', 139],\n",
              " ['<0x89>', 140],\n",
              " ['<0x8A>', 141],\n",
              " ['<0x8B>', 142],\n",
              " ['<0x8C>', 143],\n",
              " ['<0x8D>', 144],\n",
              " ['<0x8E>', 145],\n",
              " ['<0x8F>', 146],\n",
              " ['<0x90>', 147],\n",
              " ['<0x91>', 148],\n",
              " ['<0x92>', 149],\n",
              " ['<0x93>', 150],\n",
              " ['<0x94>', 151],\n",
              " ['<0x95>', 152],\n",
              " ['<0x96>', 153],\n",
              " ['<0x97>', 154],\n",
              " ['<0x98>', 155],\n",
              " ['<0x99>', 156],\n",
              " ['<0x9A>', 157],\n",
              " ['<0x9B>', 158],\n",
              " ['<0x9C>', 159],\n",
              " ['<0x9D>', 160],\n",
              " ['<0x9E>', 161],\n",
              " ['<0x9F>', 162],\n",
              " ['<0xA0>', 163],\n",
              " ['<0xA1>', 164],\n",
              " ['<0xA2>', 165],\n",
              " ['<0xA3>', 166],\n",
              " ['<0xA4>', 167],\n",
              " ['<0xA5>', 168],\n",
              " ['<0xA6>', 169],\n",
              " ['<0xA7>', 170],\n",
              " ['<0xA8>', 171],\n",
              " ['<0xA9>', 172],\n",
              " ['<0xAA>', 173],\n",
              " ['<0xAB>', 174],\n",
              " ['<0xAC>', 175],\n",
              " ['<0xAD>', 176],\n",
              " ['<0xAE>', 177],\n",
              " ['<0xAF>', 178],\n",
              " ['<0xB0>', 179],\n",
              " ['<0xB1>', 180],\n",
              " ['<0xB2>', 181],\n",
              " ['<0xB3>', 182],\n",
              " ['<0xB4>', 183],\n",
              " ['<0xB5>', 184],\n",
              " ['<0xB6>', 185],\n",
              " ['<0xB7>', 186],\n",
              " ['<0xB8>', 187],\n",
              " ['<0xB9>', 188],\n",
              " ['<0xBA>', 189],\n",
              " ['<0xBB>', 190],\n",
              " ['<0xBC>', 191],\n",
              " ['<0xBD>', 192],\n",
              " ['<0xBE>', 193],\n",
              " ['<0xBF>', 194],\n",
              " ['<0xC0>', 195],\n",
              " ['<0xC1>', 196],\n",
              " ['<0xC2>', 197],\n",
              " ['<0xC3>', 198],\n",
              " ['<0xC4>', 199],\n",
              " ['<0xC5>', 200],\n",
              " ['<0xC6>', 201],\n",
              " ['<0xC7>', 202],\n",
              " ['<0xC8>', 203],\n",
              " ['<0xC9>', 204],\n",
              " ['<0xCA>', 205],\n",
              " ['<0xCB>', 206],\n",
              " ['<0xCC>', 207],\n",
              " ['<0xCD>', 208],\n",
              " ['<0xCE>', 209],\n",
              " ['<0xCF>', 210],\n",
              " ['<0xD0>', 211],\n",
              " ['<0xD1>', 212],\n",
              " ['<0xD2>', 213],\n",
              " ['<0xD3>', 214],\n",
              " ['<0xD4>', 215],\n",
              " ['<0xD5>', 216],\n",
              " ['<0xD6>', 217],\n",
              " ['<0xD7>', 218],\n",
              " ['<0xD8>', 219],\n",
              " ['<0xD9>', 220],\n",
              " ['<0xDA>', 221],\n",
              " ['<0xDB>', 222],\n",
              " ['<0xDC>', 223],\n",
              " ['<0xDD>', 224],\n",
              " ['<0xDE>', 225],\n",
              " ['<0xDF>', 226],\n",
              " ['<0xE0>', 227],\n",
              " ['<0xE1>', 228],\n",
              " ['<0xE2>', 229],\n",
              " ['<0xE3>', 230],\n",
              " ['<0xE4>', 231],\n",
              " ['<0xE5>', 232],\n",
              " ['<0xE6>', 233],\n",
              " ['<0xE7>', 234],\n",
              " ['<0xE8>', 235],\n",
              " ['<0xE9>', 236],\n",
              " ['<0xEA>', 237],\n",
              " ['<0xEB>', 238],\n",
              " ['<0xEC>', 239],\n",
              " ['<0xED>', 240],\n",
              " ['<0xEE>', 241],\n",
              " ['<0xEF>', 242],\n",
              " ['<0xF0>', 243],\n",
              " ['<0xF1>', 244],\n",
              " ['<0xF2>', 245],\n",
              " ['<0xF3>', 246],\n",
              " ['<0xF4>', 247],\n",
              " ['<0xF5>', 248],\n",
              " ['<0xF6>', 249],\n",
              " ['<0xF7>', 250],\n",
              " ['<0xF8>', 251],\n",
              " ['<0xF9>', 252],\n",
              " ['<0xFA>', 253],\n",
              " ['<0xFB>', 254],\n",
              " ['<0xFC>', 255],\n",
              " ['<0xFD>', 256],\n",
              " ['<0xFE>', 257],\n",
              " ['<0xFF>', 258],\n",
              " ['‚ñÅa', 259],\n",
              " ['‚ñÅt', 260],\n",
              " ['in', 261],\n",
              " ['‚ñÅI', 262],\n",
              " ['‚ñÅw', 263],\n",
              " ['he', 264],\n",
              " ['re', 265],\n",
              " ['‚ñÅm', 266],\n",
              " ['‚ñÅs', 267],\n",
              " ['ing', 268],\n",
              " ['‚ñÅan', 269],\n",
              " ['at', 270],\n",
              " ['‚ñÅl', 271],\n",
              " ['ou', 272],\n",
              " ['‚ñÅb', 273],\n",
              " ['‚ñÅn', 274],\n",
              " ['‚ñÅto', 275],\n",
              " ['or', 276],\n",
              " ['‚ñÅh', 277],\n",
              " ['as', 278],\n",
              " ['is', 279],\n",
              " ['ke', 280],\n",
              " ['‚ñÅd', 281],\n",
              " ['‚ñÅo', 282],\n",
              " ['hat', 283],\n",
              " ['‚ñÅand', 284],\n",
              " ['‚ñÅthe', 285],\n",
              " ['er', 286],\n",
              " ['es', 287],\n",
              " ['le', 288],\n",
              " ['on', 289],\n",
              " ['se', 290],\n",
              " ['‚ñÅe', 291],\n",
              " ['‚ñÅis', 292],\n",
              " ['‚ñÅno', 293],\n",
              " ['ar', 294],\n",
              " ['ep', 295],\n",
              " ['et', 296],\n",
              " ['gh', 297],\n",
              " ['ld', 298],\n",
              " ['ll', 299],\n",
              " ['me', 300],\n",
              " ['th', 301],\n",
              " ['‚ñÅg', 302],\n",
              " ['‚ñÅli', 303],\n",
              " ['‚ñÅof', 304],\n",
              " ['ed', 305],\n",
              " ['nt', 306],\n",
              " ['‚ñÅM', 307],\n",
              " ['‚ñÅc', 308],\n",
              " ['‚ñÅf', 309],\n",
              " ['‚ñÅu', 310],\n",
              " ['ake', 311],\n",
              " ['ght', 312],\n",
              " ['ith', 313],\n",
              " ['‚ñÅat', 314],\n",
              " ['‚ñÅha', 315],\n",
              " ['‚ñÅin', 316],\n",
              " ['‚ñÅlo', 317],\n",
              " ['‚ñÅst', 318],\n",
              " ['ight', 319],\n",
              " ['leep', 320],\n",
              " ['ning', 321],\n",
              " ['‚ñÅany', 322],\n",
              " ['‚ñÅthat', 323],\n",
              " ['‚ñÅwith', 324],\n",
              " ['‚ñÅsleep', 325],\n",
              " ['PM', 326],\n",
              " ['il', 327],\n",
              " ['ow', 328],\n",
              " ['pl', 329],\n",
              " ['su', 330],\n",
              " ['ut', 331],\n",
              " ['ve', 332],\n",
              " ['ye', 333],\n",
              " ['‚ñÅA', 334],\n",
              " ['‚ñÅK', 335],\n",
              " ['‚ñÅY', 336],\n",
              " ['‚ñÅp', 337],\n",
              " ['‚ñÅv', 338],\n",
              " ['all', 339],\n",
              " ['ame', 340],\n",
              " ['ast', 341],\n",
              " ['ble', 342],\n",
              " ['ere', 343],\n",
              " ['ery', 344],\n",
              " ['hes', 345],\n",
              " ['iet', 346],\n",
              " ['ion', 347],\n",
              " ['ore', 348],\n",
              " ['out', 349],\n",
              " ['ter', 350],\n",
              " ['‚ñÅMy', 351],\n",
              " ['‚ñÅPM', 352],\n",
              " ['‚ñÅby', 353],\n",
              " ['‚ñÅca', 354],\n",
              " ['‚ñÅme', 355],\n",
              " ['‚ñÅmy', 356],\n",
              " ['‚ñÅtr', 357],\n",
              " ['ally', 358],\n",
              " ['oing', 359],\n",
              " ['‚ñÅ', 360],\n",
              " ['e', 361],\n",
              " ['t', 362],\n",
              " ['a', 363],\n",
              " ['o', 364],\n",
              " ['n', 365],\n",
              " ['s', 366],\n",
              " ['i', 367],\n",
              " ['h', 368],\n",
              " ['r', 369],\n",
              " ['l', 370],\n",
              " ['m', 371],\n",
              " ['u', 372],\n",
              " ['g', 373],\n",
              " ['w', 374],\n",
              " ['y', 375],\n",
              " ['d', 376],\n",
              " ['I', 377],\n",
              " ['.', 378],\n",
              " [',', 379],\n",
              " ['b', 380],\n",
              " ['f', 381],\n",
              " ['k', 382],\n",
              " ['p', 383],\n",
              " ['c', 384],\n",
              " [\"'\", 385],\n",
              " ['v', 386],\n",
              " ['M', 387],\n",
              " ['1', 388],\n",
              " ['3', 389],\n",
              " ['A', 390],\n",
              " ['K', 391],\n",
              " ['P', 392],\n",
              " ['Y', 393],\n",
              " ['-', 394],\n",
              " ['8', 395],\n",
              " ['J', 396],\n",
              " ['T', 397],\n",
              " ['q', 398],\n",
              " ['x', 399]]"
            ]
          },
          "metadata": {},
          "execution_count": 46
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "ids = sp.encode(\"hello ÏïàÎÖïÌïòÏÑ∏Ïöî\")\n",
        "print(ids)\n",
        "print([sp.id_to_piece(idx) for idx in ids])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mD0E1SA3g9Sp",
        "outputId": "b0535ca4-667f-4356-e968-d8e8198d31de"
      },
      "execution_count": 47,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[360, 264, 299, 364, 360, 239, 152, 139, 238, 136, 152, 240, 152, 155, 239, 135, 187, 239, 157, 151]\n",
            "['‚ñÅ', 'he', 'll', 'o', '‚ñÅ', '<0xEC>', '<0x95>', '<0x88>', '<0xEB>', '<0x85>', '<0x95>', '<0xED>', '<0x95>', '<0x98>', '<0xEC>', '<0x84>', '<0xB8>', '<0xEC>', '<0x9A>', '<0x94>']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## MinBPE Exercise\n",
        "\n"
      ],
      "metadata": {
        "id": "f4BnKXdyiuwA"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Step 1\n",
        "\n",
        "Write the `BasicTokenizer` class, with the following three core functions:\n",
        "\n",
        "- `def train(self, text, vocab_size, verbose=False)`\n",
        "- `def encode(self, text)`\n",
        "- `def decode(self, ids)`\n",
        "\n",
        "Train your tokenizer on whatever text you like and visualize the merged tokens. Do they look reasonable? One default test you may wish to use is the text file `tests/taylorswift.txt`."
      ],
      "metadata": {
        "id": "GIf90vSei8Jh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class BasicTokenizer:\n",
        "  def __init__(self):\n",
        "    self.merges = {}\n",
        "    self.pattern = \"\"\n",
        "    self.special = {}\n",
        "    self.vocab = None\n",
        "\n",
        "  def get_pairs(tokens):\n",
        "    counts = {}\n",
        "    for p in zip(tokens, tokens[1:]):\n",
        "      counts[p] = counts.get(p, 0) + 1\n",
        "    return counts\n",
        "\n",
        "  def merge(tokens, pair, newtoken):\n",
        "    # In the token list, replace all pairs with newtoken.\n",
        "    newtokens = []\n",
        "    i = 0\n",
        "    while i < len(tokens):\n",
        "      if i < len(tokens) - 1 and tokens[i] == pair[0] and tokens[i+1] == pair[1]:\n",
        "        newtokens.append(newtoken)\n",
        "        i += 2\n",
        "      else:\n",
        "        newtokens.append(tokens[i])\n",
        "        i += 1\n",
        "    return newtokens\n",
        "\n",
        "  def train(self, text, vocab_size, verbose=False):\n",
        "    num_merges = vocab_size - 256\n",
        "    encoded_text = text.encode(\"utf-8\")\n",
        "    tokens = list(encoded_text)\n",
        "    vocab = {x: bytes([x]) for x in range(256)}\n",
        "    for i in range(num_merges):\n",
        "      newtoken = 256 + i\n",
        "      pairs = get_pairs(tokens)\n",
        "      pair = max(pairs, key=pairs.get)\n",
        "      tokens = merge(tokens, pair, newtoken)\n",
        "      merges[pair] = newtoken\n",
        "      vocab[newtoken] = vocab[pair[0]] + vocab[pair[1]]\n",
        "      if verbose:\n",
        "        print(f\"Merge {pair} -> {newtoken}\")\n",
        "\n",
        "    self.merges = merges\n",
        "    self.vocab = vocab\n",
        "\n",
        "  def encode(self, text):\n",
        "    tokens = list(text.encode(\"utf-8\"))\n",
        "    while len(tokens) > 1:\n",
        "      pairs = get_pairs(tokens)\n",
        "      pair = min(pairs, key=lambda p: pairs.get(p, float(\"inf\")))\n",
        "      if pair not in self.merges:\n",
        "        break\n",
        "      idx = self.merges[pair]\n",
        "      tokens = merge(tokens, pair, idx)\n",
        "    return tokens\n",
        "\n",
        "  def decode(self, ids):\n",
        "    tokens = b\"\".join(vocab[x] for x in ids)\n",
        "    text = tokens.decode(\"utf-8\", errors=\"replace\")\n",
        "    return text"
      ],
      "metadata": {
        "id": "pUN_1xw_lD7y"
      },
      "execution_count": 49,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Step 2\n",
        "\n",
        "Convert your `BasicTokenizer` into a `RegexTokenizer`, which takes a regex pattern and splits the text exactly as GPT-4 would. Process the parts separately as before, then concatenate the results. Retrain your tokenizer and compare the results before and after. You should see that you will now have no tokens that go across categories (numbers, letters, punctuation, more than one whitespace). Use the GPT-4 pattern:\n",
        "\n",
        "```\n",
        "GPT4_SPLIT_PATTERN = r\"\"\"'(?i:[sdmt]|ll|ve|re)|[^\\r\\n\\p{L}\\p{N}]?+\\p{L}+|\\p{N}{1,3}| ?[^\\s\\p{L}\\p{N}]++[\\r\\n]*|\\s*[\\r\\n]|\\s+(?!\\S)|\\s+\"\"\"\n",
        "```\n"
      ],
      "metadata": {
        "id": "Rbfs_uwsi-JS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "GPT4_SPLIT_PATTERN = r\"\"\"'(?i:[sdmt]|ll|ve|re)|[^\\r\\n\\p{L}\\p{N}]?+\\p{L}+|\\p{N}{1,3}| ?[^\\s\\p{L}\\p{N}]++[\\r\\n]*|\\s*[\\r\\n]|\\s+(?!\\S)|\\s+\"\"\""
      ],
      "metadata": {
        "id": "Lb88lNvHu4M8"
      },
      "execution_count": 50,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# NOT COMPLETE\n",
        "\n",
        "class RegexTokenizer:\n",
        "  def __init__(self):\n",
        "    self.merges = {}\n",
        "    self.pattern = GPT4_SPLIT_PATTERN if pattern is None else pattern\n",
        "    self.compiled_pattern = re.compile(self.pattern)\n",
        "    self.special = {}\n",
        "    self.inverse = {}\n",
        "\n",
        "  def get_pairs(tokens):\n",
        "    counts = {}\n",
        "    for p in zip(tokens, tokens[1:]):\n",
        "      counts[p] = counts.get(p, 0) + 1\n",
        "    return counts\n",
        "\n",
        "  def merge(tokens, pair, newtoken):\n",
        "    # In the token list, replace all pairs with newtoken.\n",
        "    newtokens = []\n",
        "    i = 0\n",
        "    while i < len(tokens):\n",
        "      if i < len(tokens) - 1 and tokens[i] == pair[0] and tokens[i+1] == pair[1]:\n",
        "        newtokens.append(newtoken)\n",
        "        i += 2\n",
        "      else:\n",
        "        newtokens.append(tokens[i])\n",
        "        i += 1\n",
        "    return newtokens\n",
        "\n",
        "  def register_special_tokens(self, special_tokens):\n",
        "    self.special = special_tokens\n",
        "    self.inverse = {v: k for k, v in special_tokens.items()}\n",
        "\n",
        "  def train(self, text, vocab_size, verbose=False):\n",
        "    num_merges = vocab_size - 256\n",
        "    encoded_text = text.encode(\"utf-8\")\n",
        "    chunks = re.findall(self.compiled_pattern, text)\n",
        "    tokens = [list(chunk.encode(\"utf-8\") for chunk in chunks)]\n",
        "    merges = {}\n",
        "    vocab = {x: bytes([x]) for x in range(256)}\n",
        "    for i in range(num_merges):\n",
        "      t = {}\n",
        "      for chunk in tokens:\n",
        "        pair(chunk, t)\n",
        "      newtoken = 256 + i\n",
        "      pairs = get_pairs(tokens)\n",
        "      pair = max(pairs, key=pairs.get)\n",
        "      tokens = [merge(chunk, pair, newtoken) for chunk in tokens]\n",
        "      merges[pair] = newtoken\n",
        "      vocab[newtoken] = vocab[pair[0]] + vocab[pair[1]]\n",
        "      if verbose:\n",
        "        print(f\"Merge {pair} -> {newtoken}\")\n",
        "\n",
        "    self.merges = merges\n",
        "    self.vocab = vocab\n",
        "\n",
        "  def encode(self, text):\n",
        "    tokens = list(text.encode(\"utf-8\"))\n",
        "    while len(tokens) > 1:\n",
        "      pairs = get_pairs(tokens)\n",
        "      pair = min(pairs, key=lambda p: pairs.get(p, float(\"inf\")))\n",
        "      if pair not in pairs:\n",
        "        break\n",
        "      idx = pairs[pair]\n",
        "      tokens = merge(tokens, pair, idx)\n",
        "    return tokens\n",
        "\n",
        "  def decode(self, ids):\n",
        "    tokens = b\"\".join(vocab[x] for x in ids)\n",
        "    text = tokens.decode(\"utf-8\", errors=\"replace\")\n",
        "    return text"
      ],
      "metadata": {
        "id": "l23aIvQnusjc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Step 3\n",
        "\n",
        "You're now ready to load the merges from the GPT-4 tokenizer and show that your tokenizer produces the identical results for both `encode` and `decode`, matching [tiktoken](https://github.com/openai/tiktoken).\n",
        "\n",
        "```\n",
        "# match this\n",
        "import tiktoken\n",
        "enc = tiktoken.get_encoding(\"cl100k_base\") # this is the GPT-4 tokenizer\n",
        "ids = enc.encode(\"hello world!!!? (ÏïàÎÖïÌïòÏÑ∏Ïöî!) lol123 üòâ\")\n",
        "text = enc.decode(ids) # get the same text back\n",
        "```\n",
        "\n",
        "Unfortunately, you will run into two issues:\n",
        "\n",
        "1. It is not trivial to recover the raw merges from the GPT-4 tokenizer. You can easily recover what we call `vocab` here, and what they call and store under `enc._mergeable_ranks`. Feel free to copy paste the `recover_merges` function in `minbpe/gpt4.py`, which takes these ranks and returns the raw merges. If you wish to know how this function works, read [this](https://github.com/openai/tiktoken/issues/60) and [this](https://github.com/karpathy/minbpe/issues/11#issuecomment-1950805306). Basically, under some conditions it is enough to only store the parent nodes (and their rank) and get rid of the precise details of which children merged up to any parent.\n",
        "2. Second, the GPT-4 tokenizer for some reason permutes its raw bytes. It stores this permutation in the first 256 elements of the mergeable ranks, so you can recover this byte shuffle relatively simply as `byte_shuffle = {i: enc._mergeable_ranks[bytes([i])] for i in range(256)}`. In both your encode and decode, you'll have to shuffle bytes around accordingly. If you're stuck, reference the minbpe/gpt4.py` file for hints."
      ],
      "metadata": {
        "id": "PQQOgFGVjCYW"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Step 4\n",
        "\n",
        "(Optional, irritating, not obviously useful) Add the ability to handle special tokens. You'll then be able to match the output of tiktoken even when special tokens are present, e.g.:\n",
        "\n",
        "```\n",
        "import tiktoken\n",
        "enc = tiktoken.get_encoding(\"cl100k_base\") # this is the GPT-4 tokenizer\n",
        "ids = enc.encode(\"<|endoftext|>hello world\", allowed_special=\"all\")\n",
        "```\n",
        "\n",
        "Without `allowed_special` tiktoken will error."
      ],
      "metadata": {
        "id": "nWBSFOepjF6D"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Step 5\n",
        "\n",
        "If you've made it this far, you're now a pro at LLM Tokenization! Sadly, you're not exactly done yet because a lot of LLMs outside of OpenAI (e.g. Llama, Mistral) use [sentencepiece](https://github.com/google/sentencepiece) instead. Primary difference being that sentencepiece runs BPE directly on Unicode code points instead of on UTF-8 encoded bytes. Feel free to explore sentencepiece on your own (good luck, it's not too pretty), and stretch goal if you really experience and suffer from the burden of time, re-write your BPE to be on Unicode code points and match the Llama 2 tokenizer."
      ],
      "metadata": {
        "id": "GzLgLhytjHqY"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Test Cases for Exercises"
      ],
      "metadata": {
        "id": "WGxvEsHRuMC5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pytest"
      ],
      "metadata": {
        "id": "YQSLEUYZhJyS"
      },
      "execution_count": 48,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "test_strings = [\n",
        "    \"\", # empty string\n",
        "    \"?\", # single character\n",
        "    \"hello world!!!? (ÏïàÎÖïÌïòÏÑ∏Ïöî!) lol123 üòâ\", # fun small string\n",
        "    \"FILE:taylorswift.txt\", # FILE: is handled as a special string in unpack()\n",
        "]\n",
        "def unpack(text):\n",
        "    # we do this because `pytest -v .` prints the arguments to console, and we don't\n",
        "    # want to print the entire contents of the file, it creates a mess. So here we go.\n",
        "    if text.startswith(\"FILE:\"):\n",
        "        dirname = os.path.dirname(os.path.abspath(__file__))\n",
        "        taylorswift_file = os.path.join(dirname, text[5:])\n",
        "        contents = open(taylorswift_file, \"r\", encoding=\"utf-8\").read()\n",
        "        return contents\n",
        "    else:\n",
        "        return text\n",
        "\n",
        "specials_string = \"\"\"\n",
        "<|endoftext|>Hello world this is one document\n",
        "<|endoftext|>And this is another document\n",
        "<|endoftext|><|fim_prefix|>And this one has<|fim_suffix|> tokens.<|fim_middle|> FIM\n",
        "<|endoftext|>Last document!!! üëã<|endofprompt|>\n",
        "\"\"\".strip()\n",
        "special_tokens = {\n",
        "    '<|endoftext|>': 100257,\n",
        "    '<|fim_prefix|>': 100258,\n",
        "    '<|fim_middle|>': 100259,\n",
        "    '<|fim_suffix|>': 100260,\n",
        "    '<|endofprompt|>': 100276\n",
        "}\n",
        "llama_text = \"\"\"\n",
        "<|endoftext|>The llama (/Ààl…ëÀêm…ô/; Spanish pronunciation: [Àà éama] or [Àà ùama]) (Lama glama) is a domesticated South American camelid, widely used as a meat and pack animal by Andean cultures since the pre-Columbian era.\n",
        "Llamas are social animals and live with others as a herd. Their wool is soft and contains only a small amount of lanolin.[2] Llamas can learn simple tasks after a few repetitions. When using a pack, they can carry about 25 to 30% of their body weight for 8 to 13 km (5‚Äì8 miles).[3] The name llama (in the past also spelled \"lama\" or \"glama\") was adopted by European settlers from native Peruvians.[4]\n",
        "The ancestors of llamas are thought to have originated from the Great Plains of North America about 40 million years ago, and subsequently migrated to South America about three million years ago during the Great American Interchange. By the end of the last ice age (10,000‚Äì12,000 years ago), camelids were extinct in North America.[3] As of 2007, there were over seven million llamas and alpacas in South America and over 158,000 llamas and 100,000 alpacas, descended from progenitors imported late in the 20th century, in the United States and Canada.[5]\n",
        "<|fim_prefix|>In Aymara mythology, llamas are important beings. The Heavenly Llama is said to drink water from the ocean and urinates as it rains.[6] According to Aymara eschatology,<|fim_suffix|> where they come from at the end of time.[6]<|fim_middle|> llamas will return to the water springs and ponds<|endofprompt|>\n",
        "\"\"\".strip()\n",
        "\n",
        "# -----------------------------------------------------------------------------\n",
        "# tests\n",
        "\n",
        "# test encode/decode identity for a few different strings\n",
        "@pytest.mark.parametrize(\"tokenizer_factory\", [BasicTokenizer, RegexTokenizer, GPT4Tokenizer])\n",
        "@pytest.mark.parametrize(\"text\", test_strings)\n",
        "def test_encode_decode_identity(tokenizer_factory, text):\n",
        "    text = unpack(text)\n",
        "    tokenizer = tokenizer_factory()\n",
        "    ids = tokenizer.encode(text)\n",
        "    decoded = tokenizer.decode(ids)\n",
        "    assert text == decoded\n",
        "\n",
        "# test that our tokenizer matches the official GPT-4 tokenizer\n",
        "@pytest.mark.parametrize(\"text\", test_strings)\n",
        "def test_gpt4_tiktoken_equality(text):\n",
        "    text = unpack(text)\n",
        "    tokenizer = GPT4Tokenizer()\n",
        "    enc = tiktoken.get_encoding(\"cl100k_base\")\n",
        "    tiktoken_ids = enc.encode(text)\n",
        "    gpt4_tokenizer_ids = tokenizer.encode(text)\n",
        "    assert gpt4_tokenizer_ids == tiktoken_ids\n",
        "\n",
        "# test the handling of special tokens\n",
        "def test_gpt4_tiktoken_equality_special_tokens():\n",
        "    tokenizer = GPT4Tokenizer()\n",
        "    enc = tiktoken.get_encoding(\"cl100k_base\")\n",
        "    tiktoken_ids = enc.encode(specials_string, allowed_special=\"all\")\n",
        "    gpt4_tokenizer_ids = tokenizer.encode(specials_string, allowed_special=\"all\")\n",
        "    assert gpt4_tokenizer_ids == tiktoken_ids\n",
        "\n",
        "# reference test to add more tests in the future\n",
        "@pytest.mark.parametrize(\"tokenizer_factory\", [BasicTokenizer, RegexTokenizer])\n",
        "def test_wikipedia_example(tokenizer_factory):\n",
        "    \"\"\"\n",
        "    Quick unit test, following along the Wikipedia example:\n",
        "    https://en.wikipedia.org/wiki/Byte_pair_encoding\n",
        "\n",
        "    According to Wikipedia, running bpe on the input string:\n",
        "    \"aaabdaaabac\"\n",
        "\n",
        "    for 3 merges will result in string:\n",
        "    \"XdXac\"\n",
        "\n",
        "    where:\n",
        "    X=ZY\n",
        "    Y=ab\n",
        "    Z=aa\n",
        "\n",
        "    Keep in mind that for us a=97, b=98, c=99, d=100 (ASCII values)\n",
        "    so Z will be 256, Y will be 257, X will be 258.\n",
        "\n",
        "    So we expect the output list of ids to be [258, 100, 258, 97, 99]\n",
        "    \"\"\"\n",
        "    tokenizer = tokenizer_factory()\n",
        "    text = \"aaabdaaabac\"\n",
        "    tokenizer.train(text, 256 + 3)\n",
        "    ids = tokenizer.encode(text)\n",
        "    assert ids == [258, 100, 258, 97, 99]\n",
        "    assert tokenizer.decode(tokenizer.encode(text)) == text\n",
        "\n",
        "@pytest.mark.parametrize(\"special_tokens\", [{}, special_tokens])\n",
        "def test_save_load(special_tokens):\n",
        "    # take a bit more complex piece of text and train the tokenizer, chosen at random\n",
        "    text = llama_text\n",
        "    # create a Tokenizer and do 64 merges\n",
        "    tokenizer = RegexTokenizer()\n",
        "    tokenizer.train(text, 256 + 64)\n",
        "    tokenizer.register_special_tokens(special_tokens)\n",
        "    # verify that decode(encode(x)) == x\n",
        "    assert tokenizer.decode(tokenizer.encode(text, \"all\")) == text\n",
        "    # verify that save/load work as expected\n",
        "    ids = tokenizer.encode(text, \"all\")\n",
        "    # save the tokenizer (TODO use a proper temporary directory)\n",
        "    tokenizer.save(\"test_tokenizer_tmp\")\n",
        "    # re-load the tokenizer\n",
        "    tokenizer = RegexTokenizer()\n",
        "    tokenizer.load(\"test_tokenizer_tmp.model\")\n",
        "    # verify that decode(encode(x)) == x\n",
        "    assert tokenizer.decode(ids) == text\n",
        "    assert tokenizer.decode(tokenizer.encode(text, \"all\")) == text\n",
        "    assert tokenizer.encode(text, \"all\") == ids\n",
        "    # delete the temporary files\n",
        "    for file in [\"test_tokenizer_tmp.model\", \"test_tokenizer_tmp.vocab\"]:\n",
        "        os.remove(file)\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    pytest.main()"
      ],
      "metadata": {
        "id": "CbkpGXUeuV5t"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}